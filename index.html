<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
    	<meta name="description" content="ViA: View-invariant Skeleton Action Representation Learning via Motion Retargeting">

    	<title>ViA: View-invariant Skeleton Action Representation Learning via <br>
			Motion Retargeting</title>
		
    	<link href="css/bootstrap.min.css" rel="stylesheet">
    	<link href="css/style.css" rel="stylesheet">
    </head>

    <body>

	<div class="container">
      <div class="header">
        <h2><center> 
			ViA: View-invariant Skeleton Action Representation Learning via 
			Motion Retargeting
		
		<h3><center></center></h3>
		<h4> <center> <a href="https://www.linkedin.com/in/di-yang-38479a174/" class="text-info"> Di Yang </a>  &nbsp&nbsp&nbsp&nbsp  <a href="http://www-sop.inria.fr/members/Yaohui.Wang" class="text-info"> Yaohui Wang </a>  &nbsp&nbsp&nbsp&nbsp <a href="http://antitza.com" class="text-info">Antitza Dantcheva</a> &nbsp&nbsp&nbsp&nbsp <a href="https://www.linkedin.com/in/lorenzo-garattoni/" class="text-info">Lorenzo Garattoni</a> &nbsp&nbsp&nbsp&nbsp <a href="https://iridia.ulb.ac.be/~gfrancesca/Gianpiero_Francesca/Home.html" class="text-info">Gianpiero Francesca </a> &nbsp&nbsp&nbsp&nbsp <a href="http://www-sop.inria.fr/members/Francois.Bremond/" class="text-info">François Brémond</a> </center> </h4>
		<h4> <center>Inria, &nbsp&nbsp Université Côte d&#39Azur, &nbsp&nbsp Toyota Motor Europe, &nbsp&nbsp Shanghai AI laboratory</center> </h4>
	  </div>

	<div class="row">
    <hr>
    <h2><center>Abstract</center></h2>
    <h4>In this work, we introduce ViA, a novel View-Invariant Autoencoder for self-supervised skeleton action representation learning. ViA leverages motion retargeting between different human performers as a pretext task, in order to disentangle the latent action-specific `Motion' features on top of the visual representation of a 2D or 3D skeleton sequence. Such `Motion' features are invariant to skeleton geometry and camera view and allow ViA to facilitate both, cross-subject and cross-view action classification tasks. We conduct a study focusing on transfer-learning for skeleton-based action recognition with self-supervised pre-training on real-world data (e.g., Posetics). Our results showcase that skeleton representations learned from ViA are generic enough to improve upon state-of-the-art action classification accuracy, not only on 3D laboratory datasets such as NTU-RGB+D 60 and NTU-RGB+D 120, but also on real-world datasets where only 2D data are accurately estimated, e.g., Toyota Smarthome, UAV-Human and Penn Action.
	</h4>
	
   <center>
   <h3><a href="https://arxiv.org/pdf/" class="text-info">[Paper]</a> &nbsp&nbsp&nbsp&nbsp <a href="https://github.com/YangDi666/UNIK" class="text-info">[Code]</a>
 	<video width="1120" height="630" autoplay="autoplay" loop="loop" muted="muted" controls>
		<source src="demos/video-via.mp4" type="video/mp4">
	</video>
	
	</center>
	</div>	
	
   
	<hr>
	</div>
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    </body>
</html>
